[{"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/constexpr_math.hpp", "name":"constexpr_math.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/constexpr_math.hpp", "content":"#ifndef __CONSTEXPR_MATH__\u000A#define __CONSTEXPR_MATH__\u000A\u000A//\u000A// This file contains various helper C++ metaprogramming math functions that\u000A// are useful across various designs.\u000A//\u000A\u000A#include <limits>\u000A#include <type_traits>\u000A\u000Anamespace fpga_tools {\u000A\u000A// returns the absolute value of 'x'\u000Atemplate <typename T>\u000Aconstexpr T Abs(T x) { return (x < 0) ? -x : x; }\u000A\u000A// returns the minimum of 'a' and 'b'.\u000A// The type, 'T', must have an operator<\u000Atemplate <typename T>\u000Aconstexpr T Min(T a, T b) { return (a < b) ? a : b; }\u000A\u000A// returns the maximum of 'a' and 'b'.\u000A// The type, 'T', must have an operator>\u000Atemplate <typename T>\u000Aconstexpr T Max(T a, T b) { return (a > b) ? a : b; }\u000A\u000A// rounds up to the nearest multiple of of 'multiple'\u000A// only works for positive numbers\u000Atemplate <typename T>\u000Aconstexpr T RoundUpToMultiple(T num, T multiple) {\u000A  static_assert(std::is_integral_v<T>);\u000A  static_assert(std::is_unsigned_v<T>);\u000A  if (multiple == 0) {\u000A    return num;\u000A  }\u000A\u000A  int remainder = num % multiple;\u000A  if (remainder == 0) {\u000A    return num;\u000A  } else {\u000A    return num + multiple - remainder;\u000A  }\u000A}\u000A\u000A// returns n^2\u000Atemplate <typename T>\u000Aconstexpr T Pow2(T n) {\u000A  static_assert(std::is_integral_v<T>);\u000A  return (n < 0) ? (T(1) << (-n)) : (T(1) << n);\u000A}\u000A\u000A// returns whether abs(n) is a power of 2\u000Atemplate <typename T>\u000Aconstexpr bool IsPow2(T n) {\u000A  static_assert(std::is_integral_v<T>);\u000A  if (n < 0) n = -n;\u000A  return (n != 0) && ((n & (n - 1)) == 0);\u000A}\u000A\u000A// returns log2(n) rounding down\u000Atemplate <typename T>\u000Aconstexpr T Log2(T n) {\u000A  static_assert(std::is_integral_v<T>);\u000A  if (n < 2) {\u000A    return T(0);\u000A  } else {\u000A    T ret = 0;\u000A    while (n >= 2) {\u000A      ret++;\u000A      n /= 2;\u000A    }\u000A    return ret;\u000A  }\u000A}\u000A\u000A// returns log(2) rounded up\u000Atemplate <typename T>\u000Astatic constexpr T CeilLog2(T n) {\u000A  return ((n == 1) ? T(0) : Log2(n - 1) + T(1));\u000A}\u000A\u000A\u000A// returns the number of bits required to encode all the values between 0 and N\u000Atemplate <unsigned int n>\u000Astatic constexpr unsigned int BitsForMaxValue() {\u000A  return CeilLog2(n + 1);\u000A}\u000A\u000A\u000A// return 'n' rounded up to the nearest power of 2\u000Atemplate <typename T>\u000Aconstexpr T RoundUpPow2(T n) {\u000A  static_assert(std::is_integral_v<T>);\u000A  static_assert(std::is_unsigned_v<T>);\u000A  if (n == 0) {\u000A    return 2;\u000A  } else if (IsPow2(n)) {\u000A    return n;\u000A  } else {\u000A    return T(1) << (Log2(n) + 1);\u000A  }\u000A}\u000A\u000A// computes x^y where y must be an integer (positive or negative)\u000Aconstexpr double Pow(double x, int y) {\u000A  if (y == 0) {\u000A    // x^0 = 1\u000A    return 1.0;\u000A  } else {\u000A    // handle both y < 0 and y > 0 by changing loop bound and multiply value\u000A    bool y_is_negative = (y < 0);\u000A    double mult_val = y_is_negative ? (1/x) : x;\u000A    int loop_bound = y_is_negative ? -y : y;\u000A\u000A    double ret = 1.0;\u000A    for (int i = 0; i < loop_bound; i++) {\u000A      ret *= mult_val;\u000A    }\u000A    return ret;\u000A  }\u000A}\u000A\u000A// estimates e^(x) for x >= 0 using a taylor series expansion\u000A// https://en.wikipedia.org/wiki/Taylor_series\u000Aconstexpr double Exp(double x, unsigned taylor_terms=32) {\u000A  double factorial = 1.0;\u000A  double power = 1.0;\u000A  double answer = 1.0;\u000A\u000A  for(int i = 1; i < taylor_terms-1; i++) {\u000A    power *= x;\u000A    factorial *= i;\u000A    answer += power / factorial;\u000A  }\u000A  return answer;\u000A}\u000A\u000A// Scale significand using floating-point base exponent\u000A// see: http://www.cplusplus.com/reference/cmath/scalbn/\u000Aconstexpr float Scalbn(float value, int exponent) {\u000A  if (exponent == 0) {\u000A    return value;\u000A  } else {\u000A    float ret = value;\u000A    while(exponent != 0) {\u000A      if (exponent > 0) {\u000A        ret *= 2;\u000A        exponent--;\u000A      } else {\u000A        ret /= 2;\u000A        exponent++;\u000A      }\u000A    }\u000A    return ret;\u000A  }\u000A}\u000A\u000A// extract the exponent from a 32-bit float\u000Aconstexpr int FP32ExtractExponent(float x) {\u000A  if (x == 0) {\u000A    return 0;\u000A  } else {\u000A    float ret = 0;\u000A    float abs_x = Abs(x);\u000A    while (abs_x >= 2 || abs_x < 1) {\u000A      bool abs_x_gte_2 = (abs_x >= 2);\u000A      ret += (abs_x_gte_2 ? 1 : -1);\u000A      x = (abs_x_gte_2 ? (x/2) : (x*2));\u000A      abs_x = Abs(x);\u000A    }\u000A    return ret;\u000A  }\u000A}\u000A\u000A// extract the mantissa from a 32-bit float\u000Aconstexpr int FP32ExtractMantissa(float x) {\u000A  // remove hidden 1 and bias the exponent to get integer\u000A  //#pragma clang fp contract(off)\u000A  //return (Abs(x) < std::numeric_limits<float>::infinity()) ?\u000A  //        Scalbn(Scalbn(Abs(x),-FP32ExtractExponent(x))-1,23) : 0;\u000A  return Scalbn(Scalbn(Abs(x),-FP32ExtractExponent(x))-1,23);\u000A}\u000A\u000A}  // namespace fpga_tools\u000A\u000A#endif /* __CONSTEXPR_MATH__ */\u000A"}, {"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/device_print.hpp", "name":"device_print.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/device_print.hpp", "content":"#ifndef __DEVICE_PRINT__\u000A#define __DEVICE_PRINT__\u000A\u000A#ifdef __SYCL_DEVICE_ONLY__\u000A  #define CL_CONSTANT __attribute__((opencl_constant))\u000A#else\u000A  #define CL_CONSTANT\u000A#endif\u000A#define PRINTF(format, ...) { \\\u000A            static const CL_CONSTANT char _format[] = format; \\\u000A            sycl::ext::oneapi::experimental::printf(_format, ## __VA_ARGS__); }\u000A\u000A#endif /* __DEVICE_PRINT__ */"}, {"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/exception_handler.hpp", "name":"exception_handler.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/exception_handler.hpp", "content":"#ifndef __EXCEPTIONHANDLER_HPP__\u000A#define __EXCEPTIONHANDLER_HPP__\u000A#include <sycl/sycl.hpp>\u000A#include <exception>\u000A#include <iostream>\u000A\u000Avoid exception_handler(sycl::exception_list exceptions) {\u000A  for (std::exception_ptr const &e : exceptions) {\u000A    try {\u000A      std::rethrow_exception(e);\u000A    } catch (sycl::exception const &e) {\u000A      std::cout << \"Caught asynchronous SYCL exception:\\n\"\u000A                << e.what() << std::endl;\u000A    }\u000A  }\u000A}\u000A\u000A\u000A#endif //__EXCEPTIONHANDLER_HPP__\u000A"}, {"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/memory_utils.hpp", "name":"memory_utils.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/memory_utils.hpp", "content":"#ifndef __MEMORY_UTILS_HPP__\u000A#define __MEMORY_UTILS_HPP__\u000A\u000A#include <type_traits>\u000A\u000A#include <sycl/ext/intel/fpga_extensions.hpp>\u000A#include <sycl/sycl.hpp>\u000A\u000A#include \"metaprogramming_utils.hpp\"\u000A\u000A//\u000A// The utilities in this file are used for converting streaming data to/from\u000A// memory from/to a pipe.\u000A//\u000A\u000Anamespace fpga_tools {\u000A\u000Anamespace detail {\u000A\u000A//\u000A// Helper to check if a SYCL pipe and pointer have the same base type\u000A//\u000Atemplate <typename PipeT, typename PtrT>\u000Astruct pipe_and_pointer_have_same_base {\u000A  using PipeBaseT =\u000A      std::conditional_t<fpga_tools::has_subscript_v<PipeT>,\u000A                         std::decay_t<decltype(std::declval<PipeT>()[0])>,\u000A                         PipeT>;\u000A  using PtrBaseT = std::decay_t<decltype(std::declval<PtrT>()[0])>;\u000A  static constexpr bool value = std::is_same_v<PipeBaseT, PtrBaseT>;\u000A};\u000A\u000Atemplate <typename PipeT, typename PtrT>\u000Ainline constexpr bool pipe_and_pointer_have_same_base_v =\u000A    pipe_and_pointer_have_same_base<PipeT, PtrT>::value;\u000A\u000A//\u000A// Streams data from 'in_ptr' into 'Pipe', 'elements_per_cycle' elements at a\u000A// time\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, typename PtrT>\u000Avoid MemoryToPipeRemainder(PtrT in_ptr, size_t full_count,\u000A                           size_t remainder_count) {\u000A  static_assert(fpga_tools::is_sycl_pipe_v<Pipe>);\u000A  using PipeT = decltype(Pipe::read());\u000A  static_assert(fpga_tools::has_subscript_v<PipeT>);\u000A  static_assert(fpga_tools::has_subscript_v<PtrT>);\u000A  static_assert(PipeT::size == elements_per_cycle);\u000A  static_assert(pipe_and_pointer_have_same_base_v<PipeT, PtrT>);\u000A\u000A  for (size_t i = 0; i < full_count; i++) {\u000A    PipeT pipe_data;\u000A#pragma unroll\u000A    for (int j = 0; j < elements_per_cycle; j++) {\u000A      pipe_data[j] = in_ptr[i * elements_per_cycle + j];\u000A    }\u000A    Pipe::write(pipe_data);\u000A  }\u000A\u000A  PipeT pipe_data;\u000A  for (size_t i = 0; i < remainder_count; i++) {\u000A    pipe_data[i] = in_ptr[full_count * elements_per_cycle + i];\u000A  }\u000A  Pipe::write(pipe_data);\u000A}\u000A\u000A//\u000A// Streams data from 'in_ptr' into 'Pipe', 'elements_per_cycle' elements at a\u000A// time with the guarantee that 'elements_per_cycle' is a multiple of 'count'\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, typename PtrT>\u000Avoid MemoryToPipeNoRemainder(PtrT in_ptr, size_t count) {\u000A  static_assert(fpga_tools::is_sycl_pipe_v<Pipe>);\u000A  using PipeT = decltype(Pipe::read());\u000A  static_assert(fpga_tools::has_subscript_v<PipeT>);\u000A  static_assert(fpga_tools::has_subscript_v<PtrT>);\u000A  static_assert(PipeT::size == elements_per_cycle);\u000A  static_assert(pipe_and_pointer_have_same_base_v<PipeT, PtrT>);\u000A\u000A  for (size_t i = 0; i < count; i++) {\u000A    PipeT pipe_data;\u000A#pragma unroll\u000A    for (int j = 0; j < elements_per_cycle; j++) {\u000A      pipe_data[j] = in_ptr[i * elements_per_cycle + j];\u000A    }\u000A    Pipe::write(pipe_data);\u000A  }\u000A}\u000A\u000A//\u000A// Streams data from 'Pipe' to 'out_ptr', 'elements_per_cycle' elements at a\u000A// time\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, typename PtrT>\u000Avoid PipeToMemoryRemainder(PtrT out_ptr, size_t full_count,\u000A                           size_t remainder_count) {\u000A  static_assert(fpga_tools::is_sycl_pipe_v<Pipe>);\u000A  using PipeT = decltype(Pipe::read());\u000A  static_assert(fpga_tools::has_subscript_v<PipeT>);\u000A  static_assert(fpga_tools::has_subscript_v<PtrT>);\u000A  static_assert(PipeT::size == elements_per_cycle);\u000A  static_assert(pipe_and_pointer_have_same_base_v<PipeT, PtrT>);\u000A\u000A  for (size_t i = 0; i < full_count; i++) {\u000A    auto pipe_data = Pipe::read();\u000A#pragma unroll\u000A    for (int j = 0; j < elements_per_cycle; j++) {\u000A      out_ptr[i * elements_per_cycle + j] = pipe_data[j];\u000A    }\u000A  }\u000A\u000A  auto pipe_data = Pipe::read();\u000A  for (size_t i = 0; i < remainder_count; i++) {\u000A    out_ptr[full_count * elements_per_cycle + i] = pipe_data[i];\u000A  }\u000A}\u000A\u000A//\u000A// Streams data from 'Pipe' to 'out_ptr', 'elements_per_cycle' elements at a\u000A// time with the guarantee that 'elements_per_cycle' is a multiple of 'count'\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, typename PtrT>\u000Avoid PipeToMemoryNoRemainder(PtrT out_ptr, size_t count) {\u000A  static_assert(fpga_tools::is_sycl_pipe_v<Pipe>);\u000A  using PipeT = decltype(Pipe::read());\u000A  static_assert(fpga_tools::has_subscript_v<PipeT>);\u000A  static_assert(fpga_tools::has_subscript_v<PtrT>);\u000A  static_assert(PipeT::size == elements_per_cycle);\u000A  static_assert(pipe_and_pointer_have_same_base_v<PipeT, PtrT>);\u000A\u000A  for (size_t i = 0; i < count; i++) {\u000A    auto pipe_data = Pipe::read();\u000A#pragma unroll\u000A    for (int j = 0; j < elements_per_cycle; j++) {\u000A      out_ptr[i * elements_per_cycle + j] = pipe_data[j];\u000A    }\u000A  }\u000A}\u000A\u000A}  // namespace detail\u000A\u000A//\u000A// Streams data from memory to a SYCL pipe 1 element a time\u000A//\u000Atemplate <typename Pipe, typename PtrT>\u000Avoid MemoryToPipe(PtrT in_ptr, size_t count) {\u000A  static_assert(fpga_tools::is_sycl_pipe_v<Pipe>);\u000A  using PipeT = decltype(Pipe::read());\u000A  static_assert(fpga_tools::has_subscript_v<PtrT>);\u000A  static_assert(detail::pipe_and_pointer_have_same_base_v<PipeT, PtrT>);\u000A\u000A  for (size_t i = 0; i < count; i++) {\u000A    Pipe::write(in_ptr[i]);\u000A  }\u000A}\u000A\u000A//\u000A// Streams data from memory to a SYCL pipe 'elements_per_cycle' elements a time\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, bool remainder, typename PtrT>\u000Avoid MemoryToPipe(PtrT in_ptr, size_t count) {\u000A  if constexpr (!remainder) {\u000A    // user promises there is not remainder\u000A    detail::MemoryToPipeNoRemainder<Pipe, elements_per_cycle>(in_ptr, count);\u000A  } else {\u000A    // might have a remainder and it was not specified, so calculate it\u000A    auto full_count = (count / elements_per_cycle) * elements_per_cycle;\u000A    auto remainder_count = count % elements_per_cycle;\u000A    detail::MemoryToPipeRemainder<Pipe, elements_per_cycle>(in_ptr, full_count,\u000A                                                            remainder_count);\u000A  }\u000A}\u000A\u000A//\u000A// Streams data from memory to a SYCL pipe 'elements_per_cycle' elements a time\u000A// In this version, the user has specified a the amount of remainder\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, bool remainder, typename PtrT>\u000Avoid MemoryToPipe(PtrT in_ptr, size_t full_count, size_t remainder_count) {\u000A  if constexpr (!remainder) {\u000A    // user promises there is not remainder\u000A    detail::MemoryToPipeNoRemainder<Pipe, elements_per_cycle>(in_ptr,\u000A                                                              full_count);\u000A  } else {\u000A    // might have a remainder that was specified by the user\u000A    detail::MemoryToPipeRemainder<Pipe, elements_per_cycle>(in_ptr, full_count,\u000A                                                            remainder_count);\u000A  }\u000A}\u000A\u000A//\u000A// Streams data from a SYCL pipe to memory 1 element a time\u000A//\u000Atemplate <typename Pipe, typename PtrT>\u000Avoid PipeToMemory(PtrT out_ptr, size_t count) {\u000A  using PipeT = decltype(Pipe::read());\u000A  static_assert(fpga_tools::has_subscript_v<PtrT>);\u000A  static_assert(detail::pipe_and_pointer_have_same_base_v<PipeT, PtrT>);\u000A\u000A  for (size_t i = 0; i < count; i++) {\u000A    out_ptr[i] = Pipe::read();\u000A  }\u000A}\u000A\u000A//\u000A// Streams data from a SYCL pipe to memory 'elements_per_cycle' elements a time\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, bool remainder, typename PtrT>\u000Avoid PipeToMemory(PtrT out_ptr, size_t count) {\u000A  if constexpr (!remainder) {\u000A    detail::PipeToMemoryNoRemainder<Pipe, elements_per_cycle>(out_ptr, count);\u000A  } else {\u000A    auto full_count = (count / elements_per_cycle) * elements_per_cycle;\u000A    auto remainder_count = count % elements_per_cycle;\u000A    detail::PipeToMemoryRemainder<Pipe, elements_per_cycle>(out_ptr, full_count,\u000A                                                            remainder_count);\u000A  }\u000A}\u000A\u000A//\u000A// Streams data from a SYCL pipe to memory 'elements_per_cycle' elements a time\u000A// In this version, the user has specified a the amount of remainder\u000A//\u000Atemplate <typename Pipe, int elements_per_cycle, bool remainder, typename PtrT>\u000Avoid PipeToMemory(PtrT out_ptr, size_t full_count, size_t remainder_count) {\u000A  if constexpr (!remainder) {\u000A    detail::PipeToMemoryNoRemainder<Pipe, elements_per_cycle>(out_ptr,\u000A                                                              full_count);\u000A  } else {\u000A    detail::PipeToMemoryRemainder<Pipe, elements_per_cycle>(out_ptr, full_count,\u000A                                                            remainder_count);\u000A  }\u000A}\u000A\u000A\u000A/// 1. Allocate device_memory (same num bytes as in host_vector)\u000A/// 2. Transfer data host_vector->device_memory\u000A/// 3. Return sycl::device_ptr to device_memory\u000Atemplate<typename T>\u000AT* toDevice(const std::vector<T> &host_vector, sycl::queue &q) {\u000A  T* device_data = sycl::malloc_device<T>(host_vector.size(), q);\u000A  q.copy(host_vector.data(), device_data, host_vector.size()).wait();\u000A  return device_data;\u000A}\u000Atemplate<typename T, int N>\u000AT* toDevice(const T* host_array[N], sycl::queue &q) {\u000A  T* device_data = sycl::malloc_device<T>(N, q);\u000A  q.copy(host_array, device_data, N).wait();\u000A  return device_data;\u000A}\u000Atemplate<typename T>\u000AT* toDevice(const T* host_array, const int N, sycl::queue &q) {\u000A  T* device_data = sycl::malloc_device<T>(N, q);\u000A  q.copy(host_array, device_data, N).wait();\u000A  return device_data;\u000A}\u000A\u000A}  // namespace fpga_tools\u000A\u000A#endif /* __MEMORY_UTILS_HPP__ */"}, {"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/metaprogramming_utils.hpp", "name":"metaprogramming_utils.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/metaprogramming_utils.hpp", "content":"#ifndef __METAPROGRAMMING_UTILS_HPP__\u000A#define __METAPROGRAMMING_UTILS_HPP__\u000A\u000A#include <type_traits>\u000A#include <utility>\u000A\u000A#include <sycl/ext/intel/fpga_extensions.hpp>\u000A\u000Anamespace fpga_tools {\u000A\u000A//\u000A// The code below creates the constexprs 'make_integer_range'\u000A// and 'make_index_range' these are akin to 'std::make_integer_sequence'\u000A// and 'std::make_index_sequence', respectively.\u000A// However they allow you to specificy a range and can either increment\u000A// or decrement, rather than a strict increasing sequence\u000A//\u000Atemplate <typename T, typename, T begin, bool increase>\u000Astruct integer_range_impl;\u000A\u000A// incrementing case\u000Atemplate <typename T, T... N, T begin>\u000Astruct integer_range_impl<T, std::integer_sequence<T, N...>, begin, true> {\u000A  using type = std::integer_sequence<T, N + begin...>;\u000A};\u000A\u000A// decrementing case\u000Atemplate <typename T, T... N, T begin>\u000Astruct integer_range_impl<T, std::integer_sequence<T, N...>, begin, false> {\u000A  using type = std::integer_sequence<T, begin - N...>;\u000A};\u000A\u000A// integer_range\u000Atemplate <typename T, T begin, T end>\u000Ausing integer_range = typename integer_range_impl<\u000A    T, std::make_integer_sequence<T, (begin < end) ? end - begin : begin - end>,\u000A    begin, (begin < end)>::type;\u000A\u000A//\u000A// make_integer_range\u000A//\u000A// USAGE:\u000A//    make_integer_range<int,1,10>{} ==> 1,2,...,9\u000A//    make_integer_range<int,10,1>{} ==> 10,9,...,2\u000A//\u000Atemplate <class T, T begin, T end>\u000Ausing make_integer_range = integer_range<T, begin, end>;\u000A\u000A//\u000A// make_index_range\u000A//\u000A// USAGE:\u000A//    make_index_range<1,10>{} ==> 1,2,...,9\u000A//    make_index_range<10,1>{} ==> 10,9,...,2\u000A//\u000Atemplate <std::size_t begin, std::size_t end>\u000Ausing make_index_range = integer_range<std::size_t, begin, end>;\u000A\u000A//\u000A// The code below creates the constexprs 'make_integer_pow2_sequence'\u000A// and 'make_index_pow2_sequence'. These generate the sequence\u000A// 2^0, 2^1, 2^2, ... , 2^(N-1) = 1,2,4,...,2^(N-1)\u000A//\u000Atemplate <typename T, typename>\u000Astruct integer_pow2_sequence_impl;\u000A\u000Atemplate <typename T, T... Pows>\u000Astruct integer_pow2_sequence_impl<T, std::integer_sequence<T, Pows...>> {\u000A  using type = std::integer_sequence<T, (T(1) << Pows)...>;\u000A};\u000A\u000A// integer_pow2_sequence\u000Atemplate <typename T, T N>\u000Ausing integer_pow2_sequence =\u000A    typename integer_pow2_sequence_impl<T,\u000A                                        std::make_integer_sequence<T, N>>::type;\u000A\u000A//\u000A// make_integer_pow2_sequence\u000A//\u000A// USAGE:\u000A//    make_integer_pow2_sequence<int,5>{} ==> 1,2,4,8,16\u000A//\u000Atemplate <class T, T N>\u000Ausing make_integer_pow2_sequence = integer_pow2_sequence<T, N>;\u000A\u000A//\u000A// make_index_pow2_sequence\u000A//\u000A// USAGE:\u000A//    make_index_pow2_sequence<5>{} ==> 1,2,4,8,16\u000A//\u000Atemplate <std::size_t N>\u000Ausing make_index_pow2_sequence = integer_pow2_sequence<std::size_t, N>;\u000A\u000A//\u000A// Checks for existence of subscript operator\u000A//\u000Anamespace detail {\u000Atemplate <typename... >\u000Ausing void_t = void;\u000A\u000Atemplate<class T, typename = void>\u000Astruct has_subscript_impl : std::false_type { };\u000A\u000Atemplate<typename T>\u000Astruct has_subscript_impl<T, void_t<decltype(std::declval<T>()[1])>> \u000A  : std::true_type { };\u000A}  // namespace detail\u000A\u000Atemplate <typename T>\u000Astruct has_subscript {\u000A  static constexpr bool value =\u000A    std::is_same_v<typename detail::has_subscript_impl<T>::type, std::true_type>;\u000A};\u000A\u000Atemplate <typename T>\u000Ainline constexpr bool has_subscript_v = has_subscript<T>::value;\u000A\u000A//\u000A// checks if a type is any instance of SYCL pipe\u000A//\u000Anamespace detail {\u000A\u000Atemplate<typename T>\u000Astruct is_sycl_pipe_impl : std::false_type {};\u000A\u000Atemplate<typename Id, typename T, std::size_t N>\u000Astruct is_sycl_pipe_impl<sycl::ext::intel::pipe<Id, T, N>> : std::true_type {};\u000A\u000A}  // namespace detail\u000A\u000Atemplate <typename T>\u000Astruct is_sycl_pipe {\u000A  static constexpr bool value = detail::is_sycl_pipe_impl<T>{};\u000A};\u000A\u000Atemplate <typename T>\u000Ainline constexpr bool is_sycl_pipe_v = is_sycl_pipe<T>::value;\u000A\u000A} // namespace fpga_tools\u000A\u000A#endif  /* __METAPROGRAMMING_UTILS_HPP__ */"}, {"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/pipe_utils.hpp", "name":"pipe_utils.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/pipe_utils.hpp", "content":"//==============================================================\u000A// Copyright Intel Corporation\u000A//\u000A// SPDX-License-Identifier: MIT\u000A// =============================================================\u000A#ifndef __PIPE_UTILS_HPP__\u000A#define __PIPE_UTILS_HPP__\u000A\u000A#include <sycl/sycl.hpp>\u000A#include <sycl/ext/intel/fpga_extensions.hpp>\u000A#include <utility>\u000A\u000A/*\u000A\u000AThis header defines the following utilities for use with pipes in SYCL FPGA\u000Adesigns.\u000A\u000A1. PipeArray\u000A\u000A      Create a collection of pipes that can be indexed like an array.\u000A\u000A      template <class Id,          // identifier for the pipe array\u000A                typename BaseTy,   // type to write/read for each pipe\u000A                size_t min_depth,  // minimum capacity of each pipe\u000A                size_t... dims     // depth of each dimension in the array\u000A                                   // any number of dimensions are supported\u000A                >\u000A      struct PipeArray\u000A\u000A      Example usage:\u000A    \u000A      class PipeArrayId;\u000A      constexpr int min_depth = 0;\u000A      constexpr int num_pipes = 4;\u000A      using MyPipeArray = PipeArray<PipeArrayId, int, min_depth, num_pipes>;\u000A      ...\u000A      constexpr int pipe_idx = 1;\u000A      MyPipeArray::PipeAt<pipe_idx>::read(); \u000A\u000A2. PipeDuplicator\u000A\u000A      Fan-out a single pipe write to multiple pipe instances,\u000A      each of which will receive the same data.\u000A      A blocking write will perform a blocking write to each pipe.\u000A      A non-blocking write will perform a non-blocking write to each pipe,\u000A      and set success to true only if ALL writes were successful.\u000A\u000A      Note that the special case of 0 pipe instances is supported, which can \u000A      be useful as a stub for writes to pipes that are not needed in your particular \u000A      design.\u000A\u000A      template <class Id,          // name of this PipeDuplicator\u000A                typename T,        // data type to transfer\u000A                typename... Pipes  // all pipes to send duplicated writes to\u000A                >\u000A      struct PipeDuplicator\u000A\u000A      Example usage:\u000A\u000A      class PipeID1;\u000A      class PipeID2;\u000A      using MyPipe1 = sycl::ext::intel::pipe<PipeID1, int>;\u000A      using MyPipe2 = sycl::ext::intel::pipe<PipeID2, int>;\u000A\u000A      class PipeDuplicatorID;\u000A      using MyPipeDuplicator = PipeDuplicator<PipeDuplicatorID, int, MyPipe1, MyPipe2>;\u000A      ...\u000A      MyPipeDuplicator::write(1); // write the value 1 to both MyPipe1 and MyPipe2\u000A\u000A*/\u000A\u000A// =============================================================\u000A// Internal Helper Functions/Structs\u000A// =============================================================\u000A\u000Anamespace fpga_tools {\u000Anamespace detail {\u000A\u000A// Templated classes for verifying dimensions when accessing elements in the\u000A// pipe array.\u000Atemplate <size_t dim1, size_t... dims>\u000Astruct VerifierDimLayer {\u000A  template <size_t idx1, size_t... idxs>\u000A  struct VerifierIdxLayer {\u000A    static constexpr bool IsValid() {\u000A      return idx1 < dim1 &&\u000A             (VerifierDimLayer<dims...>::template VerifierIdxLayer<\u000A                 idxs...>::IsValid());\u000A    }\u000A  };\u000A};\u000Atemplate <size_t dim>\u000Astruct VerifierDimLayer<dim> {\u000A  template <size_t idx>\u000A  struct VerifierIdxLayer {\u000A    static constexpr bool IsValid() { return idx < dim; }\u000A  };\u000A};\u000A\u000A// Templated classes to perform 'currying' write to all pipes in the array\u000A// Primary template, dummy\u000Atemplate <template <std::size_t...> class WriteFunc, typename BaseTy,\u000A          typename PartialSequence, typename... RemainingSequences>\u000Astruct write_currying {};\u000A// Induction case\u000Atemplate <template <std::size_t...> class WriteFunc, typename BaseTy,\u000A          std::size_t... I, std::size_t... J, typename... RemainingSequences>\u000Astruct write_currying<WriteFunc, BaseTy, std::index_sequence<I...>,\u000A                      std::index_sequence<J...>, RemainingSequences...> {\u000A  void operator()(const BaseTy &data, bool &success) const {\u000A    (write_currying<WriteFunc, BaseTy, std::index_sequence<I..., J>,\u000A                    RemainingSequences...>()(data, success),\u000A     ...);\u000A  }\u000A};\u000A// Base case\u000Atemplate <template <std::size_t...> class WriteFunc, typename BaseTy,\u000A          std::size_t... I>\u000Astruct write_currying<WriteFunc, BaseTy, std::index_sequence<I...>> {\u000A  void operator()(const BaseTy &data, bool &success) const {\u000A    WriteFunc<I...>()(data, success);\u000A  }\u000A};\u000A\u000A}  // namespace detail\u000A\u000A// =============================================================\u000A// PipeArray\u000A// =============================================================\u000A\u000Atemplate <class Id,          // identifier for the pipe array\u000A          typename BaseTy,   // type to write/read for each pipe\u000A          size_t min_depth,  // minimum capacity of each pipe\u000A          size_t... dims     // depth of each dimension in the array\u000A                             // any number of dimensions are supported\u000A          >\u000Astruct PipeArray {\u000A  PipeArray() = delete;  // ensure we cannot create an instance\u000A\u000A  template <size_t... idxs>\u000A  struct StructId;  // the ID of each pipe in the array\u000A\u000A  // VerifyIndices checks that we only access pipe indicies that are in range\u000A  template <size_t... idxs>\u000A  struct VerifyIndices {\u000A    static_assert(sizeof...(idxs) == sizeof...(dims),\u000A                  \"Indexing into a PipeArray requires as many indices as \"\u000A                  \"dimensions of the PipeArray.\");\u000A    static_assert(fpga_tools::detail::VerifierDimLayer<dims...>::template\u000A                  VerifierIdxLayer<idxs...>::IsValid(),\u000A                  \"Index out of bounds\");\u000A    using VerifiedPipe =\u000A        sycl::ext::intel::pipe<StructId<idxs...>, BaseTy, min_depth>;\u000A  };\u000A\u000A  // helpers for accessing the dimensions of the pipe array\u000A  // usage:\u000A  //  MyPipeArray::GetNumDims() - number of dimensions in this pipe array\u000A  //  MyPipeArray::GetDimSize<3>() - size of dimension 3 in this pipe array\u000A  static constexpr size_t GetNumDims() { return (sizeof...(dims)); }\u000A  template <int dim_num>\u000A  static constexpr size_t GetDimSize() {\u000A    return std::get<dim_num>(dims...);\u000A  }\u000A\u000A  // PipeAt<idxs...> is used to reference a pipe at a particular index\u000A  template <size_t... idxs>\u000A  using PipeAt = typename VerifyIndices<idxs...>::VerifiedPipe;\u000A\u000A  // functor to impllement blocking write to all pipes in the array\u000A  template <std::size_t... I>\u000A  struct BlockingWriteFunc {\u000A    void operator()(const BaseTy &data, bool &success) const {\u000A      PipeAt<I...>::write(data);\u000A    }\u000A  };\u000A  // functor to impllement non-blocking write to all pipes in the array\u000A  template <std::size_t... I>\u000A  struct NonBlockingWriteFunc {\u000A    void operator()(const BaseTy &data, bool &success) const {\u000A      PipeAt<I...>::write(data, success);\u000A    }\u000A  };\u000A  // helper function for implementing write() call to all pipes in the array\u000A  template <template <std::size_t...> class WriteFunc,\u000A            typename... IndexSequences>\u000A  static void write_currying_helper(const BaseTy &data, bool &success,\u000A                                    IndexSequences...) {\u000A    fpga_tools::detail::write_currying<WriteFunc, BaseTy,\u000A                   std::index_sequence<>, IndexSequences...>()(data, success);\u000A  }\u000A\u000A  // blocking write\u000A  // write the same data to all pipes in the array using blocking writes\u000A  static void write(const BaseTy &data) {\u000A    bool success;  // temporary variable, ignored in BlockingWriteFunc\u000A    write_currying_helper<BlockingWriteFunc>(\u000A        data, success, std::make_index_sequence<dims>()...);\u000A  }\u000A\u000A  // non-blocking write\u000A  // write the same data to all pipes in the array using non-blocking writes\u000A  static void write(const BaseTy &data, bool &success) {\u000A    write_currying_helper<NonBlockingWriteFunc>(\u000A        data, success, std::make_index_sequence<dims>()...);\u000A  }\u000A\u000A};  // end of struct PipeArray\u000A\u000A// =============================================================\u000A// PipeDuplicator\u000A// =============================================================\u000A\u000A// Connect a kernel that writes to a single pipe to multiple pipe instances,\u000A// each of which will receive the same data.\u000A// A blocking write will perform a blocking write to each pipe.  A non-blocking\u000A// write will perform a non-blocking write to each pipe, and set success to\u000A// true only if ALL writes were successful.\u000A\u000A// primary template, dummy\u000Atemplate <class Id,          // name of this PipeDuplicator\u000A          typename T,        // data type to transfer\u000A          typename... Pipes  // all pipes to send duplicated writes to\u000A          >\u000Astruct PipeDuplicator {};\u000A\u000A// recursive case, write to each pipe\u000Atemplate <class Id,                   // name of this PipeDuplicator\u000A          typename T,                 // data type to transfer\u000A          typename FirstPipe,         // at least one output pipe\u000A          typename... RemainingPipes  // additional copies of the output pipe\u000A          >\u000Astruct PipeDuplicator<Id, T, FirstPipe, RemainingPipes...> {\u000A  PipeDuplicator() = delete;  // ensure we cannot create an instance\u000A\u000A  // Non-blocking write\u000A  static void write(const T &data, bool &success) {\u000A    bool local_success;\u000A    FirstPipe::write(data, local_success);\u000A    success = local_success;\u000A    PipeDuplicator<Id, T, RemainingPipes...>::write(data, local_success);\u000A    success &= local_success;\u000A  }\u000A\u000A  // Blocking write\u000A  static void write(const T &data) {\u000A    FirstPipe::write(data);\u000A    PipeDuplicator<Id, T, RemainingPipes...>::write(data);\u000A  }\u000A};\u000A\u000A// base case for recursion, no pipes to write to\u000A// also useful as a 'null' pipe, writes don't do anything\u000Atemplate <class Id,   // name of this PipeDuplicator\u000A          typename T  // data type to transfer\u000A          >\u000Astruct PipeDuplicator<Id, T> {\u000A  PipeDuplicator() = delete;  // ensure we cannot create an instance\u000A\u000A  // Non-blocking write\u000A  static void write(const T & /*data*/, bool &success) { success = true; }\u000A\u000A  // Blocking write\u000A  static void write(const T & /*data*/) {\u000A    // do nothing\u000A  }\u000A};\u000A\u000A} // namespace fpga_tools\u000A\u000A#endif /* __PIPE_UTILS_HPP__ */\u000A"}, {"path":"/home/u119070/git/elastic-sycl-hls/experiments/spmv_bram_elastic_pass_workdir/spmv_bram.ast.cpp", "name":"spmv_bram.ast.cpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/experiments/spmv_bram_elastic_pass_workdir/spmv_bram.ast.cpp", "content":"class MainKernel_AGU_0;\u000A/*\u000ARobert Szafarczyk, Glasgow, 2022\u000A\u000AMemory disambiguation kernel for C/C++/OpenCL/SYCL based HLS.\u000AStore queue with early execution of loads when all preceding stores have\u000Acalculated their addresses.\u000A*/\u000A\u000A#ifndef __LOAD_STORE_QUEUE_BRAM_HPP__\u000A#define __LOAD_STORE_QUEUE_BRAM_HPP__\u000A\u000A#include <sycl/sycl.hpp>\u000A\u000A#include <sycl/ext/intel/ac_types/ac_int.hpp>\u000A#include <sycl/ext/intel/fpga_extensions.hpp>\u000A\u000A#include \"constexpr_math.hpp\"\u000A#include \"device_print.hpp\"\u000A#include \"pipe_utils.hpp\"\u000A#include \"tuple.hpp\"\u000A#include \"unrolled_loop.hpp\"\u000A\u000Ausing namespace sycl;\u000Ausing namespace fpga_tools;\u000A\u000A// All our kernels don't have aliasing memory and don't need OpenCL ids.\u000A#ifndef KERNEL_PRAGMAS\u000A#define KERNEL_PRAGMAS [[intel::kernel_args_restrict]] [[intel::max_global_work_dim(0)]]\u000A#endif\u000A\u000Ausing addr_bram_t = int;\u000A\u000Aconstexpr addr_bram_t INVALID_BRAM_ADDR = -1;\u000A\u000Atemplate <typename T>\u000Astruct tagged_val_lsq_bram_t { T value; uint tag; bool valid; };\u000A\u000Astruct st_req_lsq_bram_t { addr_bram_t addr; uint tag; };\u000Astruct ld_req_lsq_bram_t { addr_bram_t addr; uint tag; uint ld_tag; };\u000A\u000Atemplate <typename LSQ_ID>\u000Aclass LSQ_BRAM;\u000A\u000A\u000A/// Always evaluates to true. Intended for creating artificial dependencies.\u000Atemplate <typename T>\u000A[[clang::optnone]] bool mk_dependency(T dep_src) {\u000A  volatile int tmp = reinterpret_cast<int8_t &>(dep_src);\u000A  return !((tmp << 1 == tmp) && (tmp != 0));\u000A}\u000A\u000Atemplate <typename value_t, typename ld_req_pipes, typename ld_val_pipes,\u000A          typename st_req_pipes, typename st_val_pipes, typename end_signal_pipe,\u000A          bool USE_SPECULATION, int ARRAY_SIZE, int NUM_LDS, int NUM_STS, \u000A          int LD_Q_SIZE = 4, int ST_Q_SIZE = 8>\u000A[[clang::optnone]] event LoadStoreQueueBRAM(queue &q) {\u000A  assert(LD_Q_SIZE >= 2 && \"Load queue size must be at least 2.\");\u000A  assert(ST_Q_SIZE >= 2 && \"Store queue size must be at least 2.\");\u000A  \u000A  auto lsqEvent = q.submit([&](handler &hnd) {\u000A    hnd.single_task<LSQ_BRAM<end_signal_pipe>>([=]() KERNEL_PRAGMAS {\u000A      // Dual port memory, capable of 1 rd and 1 wr per cycle.\u000A      [[intel::singlepump]] \u000A      [[intel::max_replicates(1)]] \u000A      [[intel::numbanks(1)]] \u000A      value_t DATA[ARRAY_SIZE];\u000A\u000A      // Registers for store logic.\u000A      [[intel::fpga_register]] addr_bram_t st_alloc_addr[ST_Q_SIZE];\u000A      [[intel::fpga_register]] bool st_alloc_addr_valid[ST_Q_SIZE];\u000A      [[intel::fpga_register]] uint st_alloc_tag[ST_Q_SIZE];\u000A      uint last_st_req_tag = 0;\u000A      // Initialize store allocation queue with invalid entries.\u000A      #pragma unroll\u000A      for (int i=0; i<ST_Q_SIZE; ++i) \u000A        st_alloc_addr[i] = INVALID_BRAM_ADDR;\u000A\u000A      // Registers for load logic. \u000A      [[intel::fpga_register]] addr_bram_t ld_addr[LD_Q_SIZE];\u000A      [[intel::fpga_register]] uint ld_tag[LD_Q_SIZE];\u000A      [[intel::fpga_register]] bool ld_addr_valid[LD_Q_SIZE];\u000A      [[intel::fpga_register]] bool ld_val_valid[LD_Q_SIZE];\u000A      [[intel::fpga_register]] bool ld_is_safe[LD_Q_SIZE];\u000A      [[intel::fpga_register]] uint ld_port[LD_Q_SIZE]; // used if NUM_LDS>1\u000A      // Only the head of load queue will have values.\u000A      value_t ld_val;\u000A      value_t ld_memory_val;\u000A\u000A      // When to stop the LSQ.\u000A      bool end_signal = false;\u000A      \u000A      // Used if NUM_STS > 1. st_reqs and st_vals are muxed based on tag.\u000A      st_req_lsq_bram_t st_req_read[NUM_STS];\u000A      bool st_req_read_succ[NUM_STS];\u000A      uint next_st_req_tag = 1;\u000A      tagged_val_lsq_bram_t<value_t> st_val_read[NUM_STS];\u000A      bool st_val_read_succ[NUM_STS];\u000A      uint next_st_val_tag = 1;\u000A\u000A      // Used if NUM_LDS > 1. ld_req is muxed based on the ld_tag. For a ld_req\u000A      // read from port 'k', the later ld_val will also be written to port 'k'.\u000A      ld_req_lsq_bram_t ld_req_read[NUM_LDS];\u000A      bool ld_req_read_succ[NUM_LDS];\u000A      uint next_ld_tag = 0;\u000A\u000A      [[intel::ivdep(DATA)]]\u000A      [[intel::initiation_interval(1)]] \u000A      [[intel::speculated_iterations(0)]]\u000A      while (!end_signal) {\u000A        // Only listen for end signal if store queue is empty.\u000A        if (!st_alloc_addr_valid[0]) \u000A          end_signal_pipe::read(end_signal);\u000A        \u000A        // Load from memory on every cycle. Value is discarded if not needed.\u000A        // If a load and store happen on the same cycle (possible since this is\u000A        // a pipelined loop), then the load gets the OLD_DATA (before store).\u000A        // This is configured by a parameter in the altera RAM IP.\u000A        ld_memory_val = DATA[ld_addr[0]];\u000A\u000A        /* Rule for storing to memory. */\u000A        // Store to memory if a value for a valid allocation arrives.\u000A        // The store is scheduled one cycle after the load (achieved by\u000A        // introducing an artificial constrol dependency between the loaded\u000A        // value and the store to force the scheduler.)\u000A        bool st_val_arrived = false, st_val_valid = false;\u000A        value_t st_val;\u000A        if (st_alloc_addr_valid[0]) {\u000A          if constexpr (NUM_STS == 1) {\u000A            auto _rd = st_val_pipes::template PipeAt<0>::read(st_val_arrived);\u000A            st_val = _rd.value;\u000A            // Optional support for speculative address allocations. \u000A            // If enabled, then the store value needs a valid bit to commit.\u000A            if constexpr (USE_SPECULATION) \u000A              st_val_valid = _rd.valid; \u000A            else\u000A              st_val_valid = st_val_arrived;\u000A          } else { \u000A            // Store value mux.\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (!st_val_read_succ[k])\u000A                st_val_read[k] =\u000A                    st_val_pipes::template PipeAt<k>::read(st_val_read_succ[k]);\u000A            });\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (st_val_read_succ[k] && st_val_read[k].tag == next_st_val_tag) {\u000A                st_val_read_succ[k] = false;\u000A                st_val_arrived = true;\u000A                st_val = st_val_read[k].value;\u000A                if constexpr (USE_SPECULATION) \u000A                  st_val_valid = st_val_read[k].valid;\u000A                else\u000A                  st_val_valid = true;\u000A              }\u000A            });\u000A\u000A            if (st_val_arrived) \u000A              next_st_val_tag++;\u000A          }\u000A\u000A          if (st_val_valid) {\u000A            // This is true for any value. Forces the scheduler to exec the \u000A            // store after the load.\u000A            if (mk_dependency(ld_memory_val)) \u000A              DATA[st_alloc_addr[0]] = st_val;\u000A          }\u000A        }\u000A        /* End Rule for storing to memory. */\u000A\u000A        /* Rule for returning load value to client. */\u000A        value_t bypass_val = st_val;\u000A        // We only need a bypass for the st value that arrives on this cycle.\u000A        // Any earlier st value would have already been commited to memory.\u000A        bool is_bypass = (st_val_valid && st_alloc_addr[0] == ld_addr[0] &&\u000A                          st_alloc_tag[0] == ld_tag[0]);\u000A        if (ld_addr_valid[0] && (is_bypass || ld_is_safe[0])) {\u000A          if (!ld_val_valid[0]) {\u000A            ld_val = is_bypass ? bypass_val : ld_memory_val;\u000A            ld_val_valid[0] = true;\u000A          }\u000A\u000A          bool pipe_succ = false;\u000A          if constexpr (NUM_LDS == 1) {\u000A            ld_val_pipes:: template PipeAt<0>::write(ld_val, pipe_succ);\u000A          } else {\u000A            // Write back to the correct port.\u000A            UnrolledLoop<NUM_LDS>([&](auto k) {\u000A              if (ld_port[0] == k)  \u000A                ld_val_pipes:: template PipeAt<k>::write(ld_val, pipe_succ);\u000A            });\u000A          }\u000A\u000A          if (pipe_succ) {\u000A            ld_addr_valid[0] = false;\u000A            ld_val_valid[0] = false;\u000A            ld_port[0] = -1;\u000A          }\u000A        }\u000A        /* End Rule writing the ld_return value to a pipe. */\u000A\u000A        /* Rule for reading a load requst from a pipe. */\u000A        if (!ld_addr_valid[LD_Q_SIZE-1]) {\u000A          bool ld_req_valid = false;\u000A          ld_req_lsq_bram_t ld_req;\u000A          uint next_ld_port;\u000A\u000A          if constexpr (NUM_LDS == 1) {\u000A            ld_req = ld_req_pipes::template PipeAt<0>::read(ld_req_valid);\u000A          } else { // Ld_req mux. \u000A            UnrolledLoop<NUM_LDS>([&](auto k) {\u000A              if (!ld_req_read_succ[k])\u000A                ld_req_read[k] =\u000A                    ld_req_pipes::template PipeAt<k>::read(ld_req_read_succ[k]);\u000A            });\u000A            UnrolledLoop<NUM_LDS>([&](auto k) {\u000A              if (ld_req_read_succ[k] && ld_req_read[k].ld_tag == next_ld_tag) {\u000A                ld_req_read_succ[k] = false;\u000A                ld_req = ld_req_read[k];\u000A                ld_req_valid = true;\u000A                // Record where this load request came from.\u000A                next_ld_port = k;\u000A              }\u000A            });\u000A          }\u000A\u000A          if (ld_req_valid) {\u000A            ld_addr[LD_Q_SIZE - 1] = ld_req.addr;\u000A            ld_tag[LD_Q_SIZE - 1] = ld_req.tag;\u000A            ld_addr_valid[LD_Q_SIZE - 1] = true;\u000A            ld_port[LD_Q_SIZE - 1] = next_ld_port;\u000A            next_ld_tag++;\u000A          }\u000A        }\u000A        /* End Rule for reading a load requst from a pipe. */\u000A\u000A        /* Rule for setting ld_is_safe flag (only first two entries).*/\u000A        UnrolledLoop<2>([&](auto iLd) {\u000A          // Only if ld_addr is valid, and if previous store addresses arrived.\u000A          if (ld_addr_valid[iLd] && ld_tag[iLd] <= last_st_req_tag) {\u000A            bool match[ST_Q_SIZE];\u000A            #pragma unroll\u000A            for (int i = 0; i < ST_Q_SIZE; ++i) {\u000A              match[i] = (st_alloc_addr[i] == ld_addr[iLd] && \u000A                          st_alloc_tag[i] <= ld_tag[iLd]);\u000A            }\u000A\u000A            bool ld_wait = false;\u000A            #pragma unroll\u000A            for (int i=0; i<ST_Q_SIZE; ++i)\u000A              ld_wait |= match[i];\u000A\u000A            // Once a load is marked as safe, it cannot go back to being unsafe.\u000A            if (!ld_wait) \u000A              ld_is_safe[iLd] = true;\u000A          }\u000A        });\u000A        /* End Rule for setting ld_is_safe flag.*/\u000A\u000A        // /* Rule for load queue shift.*/\u000A        if (!ld_addr_valid[0]) {\u000A          UnrolledLoop<LD_Q_SIZE - 1>([&](auto i) {\u000A            ld_addr[i] = ld_addr[i + 1];\u000A            ld_tag[i] = ld_tag[i + 1];\u000A            ld_addr_valid[i] = ld_addr_valid[i + 1];\u000A            ld_val_valid[i] = ld_val_valid[i + 1];\u000A            ld_is_safe[i] = ld_is_safe[i + 1];\u000A            ld_port[i] = ld_port[i + 1];\u000A          });\u000A          ld_addr[LD_Q_SIZE-1] = 0;\u000A          ld_tag[LD_Q_SIZE-1] = 0;\u000A          ld_addr_valid[LD_Q_SIZE-1] = false;\u000A          ld_val_valid[LD_Q_SIZE-1] = false;\u000A          ld_is_safe[LD_Q_SIZE-1] = false;\u000A          ld_port[LD_Q_SIZE-1] = 0;\u000A        }\u000A        /* End Rule for load queue shift register shift.*/\u000A\u000A        /* Rule for store request pipe read and store_q shift register shift.*/\u000A        if (st_val_arrived || !st_alloc_addr_valid[0]) {\u000A          uint next_tag;\u000A          addr_bram_t next_addr = INVALID_BRAM_ADDR;\u000A          bool st_req_valid = false;\u000A          st_req_lsq_bram_t st_req;\u000A\u000A          if constexpr (NUM_STS == 1) {\u000A            st_req = st_req_pipes:: template PipeAt<0>::read(st_req_valid);\u000A          } else { // Store request mux.\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (!st_req_read_succ[k])\u000A                st_req_read[k] =\u000A                    st_req_pipes::template PipeAt<k>::read(st_req_read_succ[k]);\u000A            });\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (st_req_read_succ[k] && st_req_read[k].tag == next_st_req_tag) {\u000A                st_req_read_succ[k] = false;\u000A                st_req = st_req_read[k];\u000A                st_req_valid = true;\u000A              }\u000A            });\u000A          }\u000A\u000A          if (st_req_valid) {\u000A            next_addr = st_req.addr;\u000A            next_tag = st_req.tag;\u000A            next_st_req_tag++;\u000A            last_st_req_tag++;\u000A          }\u000A\u000A          #pragma unroll\u000A          for (int i = 0; i < ST_Q_SIZE-1; ++i) {\u000A            st_alloc_addr[i] = st_alloc_addr[i + 1];\u000A            st_alloc_addr_valid[i] = st_alloc_addr_valid[i + 1];\u000A            st_alloc_tag[i] = st_alloc_tag[i + 1];\u000A          }\u000A          st_alloc_addr[ST_Q_SIZE - 1] = next_addr;\u000A          st_alloc_addr_valid[ST_Q_SIZE - 1] = st_req_valid;\u000A          st_alloc_tag[ST_Q_SIZE - 1] = next_tag;\u000A        }\u000A        /* End Rule for store request pipe read and store_q shift register.*/\u000A\u000A      } // End main loop\u000A    });\u000A  });\u000A\u000A  return lsqEvent;\u000A}\u000A\u000A#endif // __LOAD_STORE_QUEUE_BRAM_HPP__\u000A/*\u000ARobert Szafarczyk, Glasgow, 2022\u000A\u000ALoad Store Queue kernel for SYCL HLS. The LSQ is for off-chip memory.\u000A*/\u000A\u000A#ifndef __LOAD_STORE_QUEUE_DRAM_HPP__\u000A#define __LOAD_STORE_QUEUE_DRAM_HPP__\u000A\u000A#include <sycl/sycl.hpp>\u000A\u000A#include <sycl/ext/intel/fpga_extensions.hpp>\u000A#include <sycl/ext/intel/ac_types/ac_int.hpp>\u000A\u000A#include \"pipe_utils.hpp\"\u000A#include \"tuple.hpp\"\u000A#include \"unrolled_loop.hpp\"\u000A#include \"constexpr_math.hpp\"\u000A#include \"device_print.hpp\"\u000A\u000Ausing namespace sycl;\u000Ausing namespace fpga_tools;\u000A\u000A// All our kernels don't have aliasing memory and don't need OpenCL ids.\u000A#define KERNEL_PRAGMAS [[intel::kernel_args_restrict]] [[intel::max_global_work_dim(0)]] \u000A\u000A// Use the simplest load/store units for DRAM (no bursts, no caches, no prefetching).\u000Ausing PipelinedLSU = sycl::ext::intel::lsu<>;\u000A\u000A/// Max number of cycles between issuing store to DRAM memory controller and commit.\u000Aconstexpr int LSU_STORE_LATENCY = 8; \u000Aconstexpr int LSU_LOAD_LATENCY = 8; \u000A\u000A/// Represents pointer bits.\u000Ausing addr_dram_t = int64_t;\u000Aconstexpr addr_dram_t INVALID_ADDRESS = -1; \u000A\u000Astruct req_lsq_dram_t { addr_dram_t addr; uint tag; };\u000A\u000Atemplate <typename T>\u000Astruct addr_dram_val_pair_t { addr_dram_t addr; T value; };\u000A\u000Atemplate <typename T>\u000Astruct tagged_val_lsq_dram_t { T value; uint tag; bool valid; };\u000A\u000A// To generate unique kernel names for multiple load ports across multiple LSQs.\u000Atemplate <typename LSQ_ID, int PORT_NUM>\u000Aclass LoadPort;\u000Atemplate <typename LSQ_ID, int MUX_NUM>\u000Aclass LoadMux;\u000Atemplate <typename LSQ_ID>\u000Aclass StorePort;\u000Atemplate <typename LSQ_ID>\u000Aclass LSQ_DRAM;\u000A\u000Atemplate <typename value_t, typename ld_req_pipes, typename ld_val_pipes,\u000A          typename st_req_pipes, typename st_val_pipes,\u000A          typename end_signal_pipe, bool USE_SPECULATION, int NUM_LDS,\u000A          int NUM_STS, int LD_Q_SIZE = 4, int ST_Q_SIZE = 8>\u000A[[clang::optnone]] event LoadStoreQueueDRAM(queue &q) {\u000A  assert(LD_Q_SIZE >= 2 && \"Load queue size must be at least 2.\");\u000A  assert(ST_Q_SIZE >= 2 && \"Store queue size must be at least 2.\");\u000A\u000A  // Pipes to connect LSQ to ld/st ports and to ld value mux.\u000A  using pred_st_port_pipe = pipe<class pred_st_port_pipe_class, bool, LSU_STORE_LATENCY>;\u000A  using st_port_val_pipe = pipe<class st_port_val_pipe_class, addr_dram_val_pair_t<value_t>, LSU_STORE_LATENCY>;\u000A\u000A  // Each load gets its own load port and load mux, thus the use of PipeArrays.\u000A  using pred_ld_port_pipes = PipeArray<class pred_ld_port_pipe_class, bool, LSU_LOAD_LATENCY, NUM_LDS>;\u000A  using ld_port_addr_pipes = PipeArray<class ld_port_addr_pipe_class, addr_dram_t, LSU_LOAD_LATENCY, NUM_LDS>;\u000A  using pred_ld_mux_pipes = PipeArray<class pred_ld_mux_pipe_class, bool, LSU_LOAD_LATENCY, NUM_LDS>;\u000A  using ld_mux_sel_pipes = PipeArray<class ld_mux_sel_pipe_class, bool, LSU_LOAD_LATENCY, NUM_LDS>;\u000A  using ld_mux_from_memory_val_pipes = PipeArray<class ld_mux_from_memory_val_class, value_t, LSU_LOAD_LATENCY, NUM_LDS>;\u000A  using ld_mux_from_bypass_val_pipes = PipeArray<class ld_mux_from_bypass_val_class, value_t, LSU_LOAD_LATENCY, NUM_LDS>;\u000A\u000A  /// Store port kernel\u000A  auto storePortEvent = q.submit([&](handler &hnd) {\u000A    hnd.single_task<StorePort<end_signal_pipe>>([=]() KERNEL_PRAGMAS {\u000A      [[intel::speculated_iterations(0)]]\u000A      while (pred_st_port_pipe::read()) {\u000A        addr_dram_val_pair_t<value_t> addr_val = st_port_val_pipe::read();\u000A        auto st_addr_dram_typed =\u000A            sycl::ext::intel::device_ptr<value_t>((value_t *)addr_val.addr);\u000A        PipelinedLSU::store(st_addr_dram_typed, addr_val.value);\u000A      }\u000A    });\u000A  });\u000A\u000A  /// Compile time unroll of load port and load mux kernels.\u000A  UnrolledLoop<NUM_LDS>([&](auto iLd) {\u000A    q.submit([&](handler &hnd) {\u000A      hnd.single_task<LoadPort<end_signal_pipe, iLd>>([=]() KERNEL_PRAGMAS {\u000A        [[intel::speculated_iterations(0)]]\u000A        while (pred_ld_port_pipes:: template PipeAt<iLd>::read()) {\u000A          addr_dram_t ld_addr = ld_port_addr_pipes:: template PipeAt<iLd>::read();\u000A          auto ld_addr_dram_typed =\u000A              sycl::ext::intel::device_ptr<value_t>((value_t *)ld_addr);\u000A          value_t ld_val = PipelinedLSU::load(ld_addr_dram_typed);\u000A          ld_mux_from_memory_val_pipes:: template PipeAt<iLd>::write(ld_val);\u000A        }\u000A      });\u000A    });\u000A\u000A    q.submit([&](handler &hnd) {\u000A      hnd.single_task<LoadMux<end_signal_pipe, iLd>>([=]() KERNEL_PRAGMAS {\u000A        [[intel::speculated_iterations(0)]]\u000A        while (pred_ld_mux_pipes:: template PipeAt<iLd>::read()) {\u000A          value_t val;\u000A          if (ld_mux_sel_pipes:: template PipeAt<iLd>::read()) \u000A            val = ld_mux_from_bypass_val_pipes:: template PipeAt<iLd>::read();\u000A          else\u000A            val = ld_mux_from_memory_val_pipes:: template PipeAt<iLd>::read();\u000A          \u000A          ld_val_pipes:: template PipeAt<iLd>::write(val);\u000A        }\u000A      });\u000A    });\u000A  }); // End load port and load mux kernels\u000A\u000A  /// Load store queue kernel.\u000A  auto lsqEvent = q.submit([&](handler &hnd) {\u000A    hnd.single_task<LSQ_DRAM<end_signal_pipe>>([=]() KERNEL_PRAGMAS {\u000A      // Registers for store logic.\u000A      [[intel::fpga_register]] addr_dram_t st_alloc_addr[ST_Q_SIZE];\u000A      [[intel::fpga_register]] bool st_alloc_addr_valid[ST_Q_SIZE];\u000A      [[intel::fpga_register]] uint st_alloc_tag[ST_Q_SIZE];\u000A      [[intel::fpga_register]] addr_dram_t st_commit_addr[LSU_STORE_LATENCY];\u000A      [[intel::fpga_register]] value_t st_commit_value[LSU_STORE_LATENCY];\u000A      uint last_st_req_tag = 0;\u000A      #pragma unroll\u000A      for (int i = 0; i < LSU_STORE_LATENCY; ++i) \u000A        st_commit_addr[i] = INVALID_ADDRESS;\u000A\u000A      // Registers for load logic. \u000A      enum LD_STATE { WAIT_FOR_REQ, SEARCH, RETURN };\u000A      [[intel::fpga_register]] LD_STATE ld_state[NUM_LDS][LD_Q_SIZE];\u000A      [[intel::fpga_register]] addr_dram_t ld_addr[NUM_LDS][LD_Q_SIZE];\u000A      [[intel::fpga_register]] uint ld_tag[NUM_LDS][LD_Q_SIZE];\u000A      [[intel::fpga_register]] bool ld_no_conflict[NUM_LDS][LD_Q_SIZE];\u000A      [[intel::fpga_register]] value_t bypass_val[NUM_LDS][LD_Q_SIZE];\u000A      [[intel::fpga_register]] bool is_bypass[NUM_LDS][LD_Q_SIZE];\u000A\u000A      // Signal to stop the LSQ.\u000A      bool end_signal = false;\u000A\u000A      // Used if NUM_STS > 1. st_reqs and st_vals are muxed based on tag.\u000A      req_lsq_dram_t st_req_read[NUM_STS];\u000A      bool st_req_read_succ[NUM_STS];\u000A      uint next_st_req_tag = 1;\u000A      tagged_val_lsq_dram_t<value_t> st_val_read[NUM_STS];\u000A      bool st_val_read_succ[NUM_STS];\u000A      uint next_st_val_tag = 1;\u000A\u000A      [[intel::ivdep]] \u000A      [[intel::initiation_interval(1)]] \u000A      [[intel::speculated_iterations(0)]]\u000A      while (!end_signal) {\u000A        // We can only stop if the commit queue has cleared.\u000A        if (!st_alloc_addr_valid[0]) \u000A          end_signal_pipe::read(end_signal);\u000A\u000A        /************************* Start Store Logic **************************/\u000A        // If top of store allocation queue has a valid address, then wait for\u000A        // value. If value arrives, pass it to the store port and put it at the\u000A        // end of the store commit queue.\u000A        bool st_val_arrived = false, st_val_valid = false;\u000A        value_t st_val;\u000A        auto next_st_commit_addr = INVALID_ADDRESS;\u000A        if (st_alloc_addr_valid[0]) {\u000A          if constexpr (NUM_STS == 1) {\u000A            auto _rd = st_val_pipes::template PipeAt<0>::read(st_val_arrived);\u000A            st_val = _rd.value;\u000A            // Optional support for speculative address allocations. \u000A            // If enabled, then the store value needs a valid bit to commit.\u000A            if constexpr (USE_SPECULATION) {\u000A              if (st_val_arrived)\u000A                st_val_valid = _rd.valid; \u000A            } else {\u000A              st_val_valid = st_val_arrived;\u000A            }\u000A          } else {\u000A            // Store value mux.\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (!st_val_read_succ[k])\u000A                st_val_read[k] =\u000A                    st_val_pipes::template PipeAt<k>::read(st_val_read_succ[k]);\u000A            });\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (st_val_read_succ[k] && st_val_read[k].tag == next_st_val_tag) {\u000A                st_val_read_succ[k] = false;\u000A                st_val_arrived = true;\u000A                st_val = st_val_read[k].value;\u000A                if constexpr (USE_SPECULATION) \u000A                  st_val_valid = st_val_read[k].valid;\u000A                else\u000A                  st_val_valid = true;\u000A              }\u000A            });\u000A\u000A            if (st_val_arrived) \u000A              next_st_val_tag++;\u000A          }\u000A\u000A          if (st_val_valid) {\u000A            pred_st_port_pipe::write(1);\u000A            st_port_val_pipe::write({st_alloc_addr[0], st_val});\u000A            next_st_commit_addr = st_alloc_addr[0];\u000A          }\u000A        }\u000A\u000A        // Shift commit queue on every cycle, unless no space is required. This \u000A        // keeps the values in the queue longer than needed, increasing re-use.\u000A        if (st_val_valid || st_commit_addr[0] == INVALID_ADDRESS) {\u000A          #pragma unroll\u000A          for (int i = 0; i < LSU_STORE_LATENCY-1; ++i) {\u000A            st_commit_addr[i] = st_commit_addr[i+1];\u000A            st_commit_value[i] = st_commit_value[i+1];\u000A          }\u000A          st_commit_addr[LSU_STORE_LATENCY-1] = next_st_commit_addr;\u000A          st_commit_value[LSU_STORE_LATENCY-1] = st_val;\u000A        }\u000A\u000A        // Accept new store allocations, unless no space in allocation queue.\u000A        if (st_val_arrived || !st_alloc_addr_valid[0]) {\u000A          bool st_req_valid = false;\u000A          auto next_st_alloc_addr = INVALID_ADDRESS;\u000A          req_lsq_dram_t st_req;\u000A\u000A          if constexpr (NUM_STS == 1) {\u000A            st_req = st_req_pipes:: template PipeAt<0>::read(st_req_valid);\u000A          } else { // Store request mux.\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (!st_req_read_succ[k])\u000A                st_req_read[k] =\u000A                    st_req_pipes::template PipeAt<k>::read(st_req_read_succ[k]);\u000A            });\u000A            UnrolledLoop<NUM_STS>([&](auto k) {\u000A              if (st_req_read_succ[k] && st_req_read[k].tag == next_st_req_tag) {\u000A                st_req_read_succ[k] = false;\u000A                st_req = st_req_read[k];\u000A                st_req_valid = true;\u000A              }\u000A            });\u000A          }\u000A\u000A          if (st_req_valid) {\u000A            next_st_alloc_addr = st_req.addr;\u000A            last_st_req_tag++;\u000A            next_st_req_tag++;\u000A          }\u000A\u000A          #pragma unroll\u000A          for (int i = 0; i < ST_Q_SIZE-1; ++i) {\u000A            st_alloc_addr[i] = st_alloc_addr[i + 1];\u000A            st_alloc_addr_valid[i] = st_alloc_addr_valid[i + 1];\u000A            st_alloc_tag[i] = st_alloc_tag[i + 1];\u000A          }\u000A          st_alloc_addr[ST_Q_SIZE - 1] = next_st_alloc_addr;\u000A          st_alloc_addr_valid[ST_Q_SIZE - 1] = st_req_valid;\u000A          st_alloc_tag[ST_Q_SIZE - 1] = st_req.tag;\u000A        }\u000A        /************************** End Store Logic ***************************/\u000A\u000A\u000A        /************************* Start Load Logic **************************/\u000A        // All loads proceed in parallel.\u000A        UnrolledLoop<NUM_LDS>([&](auto iLd) {\u000A          if (ld_state[iLd][0] == LD_STATE::SEARCH && ld_no_conflict[iLd][0]) {\u000A            // Inform ld MUX to expect value from bypass pipe or ld port.\u000A            pred_ld_mux_pipes:: template PipeAt<iLd>::write(1);\u000A            ld_mux_sel_pipes:: template PipeAt<iLd>::write(is_bypass[iLd][0]);\u000A            if (is_bypass[iLd][0]) {\u000A              // If bypass, then send value directly to mux.\u000A              ld_mux_from_bypass_val_pipes::template PipeAt<iLd>::write(\u000A                  bypass_val[iLd][0]);\u000A            } else {\u000A              // Otherwise, issue load request to load port.\u000A              pred_ld_port_pipes:: template PipeAt<iLd>::write(1);\u000A              ld_port_addr_pipes:: template PipeAt<iLd>::write(ld_addr[iLd][0]);\u000A            }\u000A\u000A            ld_state[iLd][0] = LD_STATE::WAIT_FOR_REQ;\u000A          }\u000A\u000A          // Wait for load allocation if there is space in the queue.\u000A          if (ld_state[iLd][LD_Q_SIZE-1] == LD_STATE::WAIT_FOR_REQ) {\u000A            bool pipe_succ = false;\u000A            auto ld_req = ld_req_pipes:: template PipeAt<iLd>::read(pipe_succ);\u000A            if (pipe_succ) {\u000A              ld_addr[iLd][LD_Q_SIZE - 1] = ld_req.addr;\u000A              ld_tag[iLd][LD_Q_SIZE - 1] = ld_req.tag;\u000A              ld_state[iLd][LD_Q_SIZE - 1] = LD_STATE::SEARCH;\u000A            }\u000A            ld_no_conflict[iLd][LD_Q_SIZE-1] = false;\u000A          }\u000A\u000A          // Check the first 2 entries in the ld alloc queue for store conflicts.\u000A          UnrolledLoop<2>([&](auto iAlloc) {\u000A            if (ld_tag[iLd][iAlloc] <= last_st_req_tag) {\u000A              // Pipeline stage before the OR-reduction.\u000A              bool match[ST_Q_SIZE];\u000A              bool ld_wait = false;\u000A              #pragma unroll\u000A              for (int i = 0; i < ST_Q_SIZE; ++i) {\u000A                match[i] = (st_alloc_addr[i] == ld_addr[iLd][iAlloc] &&\u000A                            st_alloc_tag[i] <= ld_tag[iLd][iAlloc]);\u000A              }\u000A\u000A              #pragma unroll\u000A              for (int i = 0; i < ST_Q_SIZE; ++i) \u000A                ld_wait |= match[i];\u000A\u000A              if (!ld_wait)\u000A                ld_no_conflict[iLd][iAlloc] = true;\u000A            }\u000A\u000A            // Check iif we can possibly forward a value from the store commit\u000A            // queue. 'is_bypass' is only valid when 'no_conflict' is true, so\u000A            // check for bypass on every invocation.\u000A            is_bypass[iLd][iAlloc] = false;\u000A            #pragma unroll\u000A            for (int i = 0; i < LSU_STORE_LATENCY; ++i) {\u000A              // If multiple matches, then the most recent one wins. No need to\u000A              // check the tags since all stores in the commit queue by\u000A              // definition come before this load in program order.\u000A              if (st_commit_addr[i] == ld_addr[iLd][iAlloc]) {\u000A                is_bypass[iLd][iAlloc] = true;\u000A                bypass_val[iLd][iAlloc] = st_commit_value[i];\u000A              }\u000A            }\u000A          });\u000A\u000A          // Shift ld alloc queue whenever the top alloc has no valid request.\u000A          if (ld_state[iLd][0] == LD_STATE::WAIT_FOR_REQ) {\u000A            UnrolledLoop<LD_Q_SIZE - 1>([&](auto i) {\u000A              ld_state[iLd][i] = ld_state[iLd][i + 1];\u000A              ld_addr[iLd][i] = ld_addr[iLd][i + 1];\u000A              ld_tag[iLd][i] = ld_tag[iLd][i + 1];\u000A              ld_no_conflict[iLd][i] = ld_no_conflict[iLd][i + 1];\u000A              is_bypass[iLd][i] = is_bypass[iLd][i+1];\u000A              bypass_val[iLd][i] = bypass_val[iLd][i+1];\u000A            });\u000A            ld_state[iLd][LD_Q_SIZE-1] = LD_STATE::WAIT_FOR_REQ;\u000A            ld_no_conflict[iLd][LD_Q_SIZE-1] = false;\u000A          }\u000A        });\u000A        /************************** End Load Logic ***************************/\u000A\u000A      } // End LSQ pipelined loop.\u000A\u000A      // Send end signals to ld/st ports.\u000A      UnrolledLoop<NUM_LDS>([&](auto iLd) {\u000A        pred_ld_port_pipes:: template PipeAt<iLd>::write(0);\u000A        pred_ld_mux_pipes:: template PipeAt<iLd>::write(0);\u000A      });\u000A      pred_st_port_pipe::write(0);\u000A\u000A    });\u000A  }); // End LSQ kernel\u000A  \u000A\u000A  // Return the event for the last kernel to finish.\u000A  return storePortEvent;\u000A}\u000A\u000A#endif\u000A#include <algorithm>\u000A#include <iostream>\u000A#include <numeric>\u000A#include <random>\u000A#include <stdlib.h>\u000A#include <sycl/sycl.hpp>\u000A#include <vector>\u000A#include <sycl/ext/intel/fpga_extensions.hpp>\u000A#include \"device_print.hpp\"\u000A#include \"exception_handler.hpp\"\u000A#include \"memory_utils.hpp\"\u000Ausing namespace sycl;\u000Aclass MainKernel;\u000Aconstexpr int kM = 20;\u000A#define TEST 0\u000Adouble spmv_kernel(queue &q, std::vector<int> &h_matrix, const std::vector<int> &h_row, const std::vector<int> &h_col, const std::vector<int> &h_a, const int M) {\u000A  int *matrix_dram = fpga_tools::toDevice(h_matrix, q);\u000A  int *row = fpga_tools::toDevice(h_row, q);\u000A  int *col = fpga_tools::toDevice(h_col, q);\u000A  int *a = fpga_tools::toDevice(h_a, q);\u000Ausing pipes_ld_req_0 =             PipeArray<class pipes_ld_req_0_class, ld_req_lsq_bram_t,                    32, 2>;\u000Ausing pipes_ld_val_0 =             PipeArray<class pipes_ld_val_0_class, int,                    32, 2>;\u000Ausing pipes_st_req_0 =             PipeArray<class pipes_st_req_0_class, st_req_lsq_bram_t,                    16, 1>;\u000Ausing pipes_st_val_0 =             PipeArray<class pipes_st_val_0_class, tagged_val_lsq_bram_t<int>,                    16, 1>;\u000Ausing pipe_end_lsq_signal_0 =             pipe<class pipe_end_lsq_signal_0_class, bool, 1>;\u000Aauto event_MainKernel_AGU_0 = q.single_task<MainKernel_AGU_0>([=]() [[intel::kernel_args_restrict]] {\u000Apipes_ld_req_0::PipeAt<0>::write({});\u000Apipes_ld_req_0::PipeAt<1>::write({});\u000Apipes_st_req_0::PipeAt<0>::write({});\u000A                int matrix[kM * kM];\u000A#if TEST\u000A    for (int i = 0; i < (M * M); ++i)\u000A      matrix[i] = matrix_dram[i];\u000A#endif\u000A    int ptr = 0;\u000A    for (int k = 1; k < kM; k++) {\u000A      for (int p = 0; p < kM; p++) {\u000A        matrix[row[ptr]] += a[p] * matrix[col[ptr]];\u000A        ptr++;\u000A      }\u000A    }\u000A#if TEST\u000A    for (int i = 0; i < (M * M); ++i)\u000A      matrix_dram[i] = matrix[i];\u000A#endif\u000A        });\u000A\u000A            auto lsqEvent_0 = \u000A                LoadStoreQueueBRAM<int, pipes_ld_req_0, pipes_ld_val_0, \u000A                                    pipes_st_req_0, pipes_st_val_0, pipe_end_lsq_signal_0, 0,\u000A                                    400, 2, 1, \u000A                                    4, 16>(q);\u000A            \u000A  auto event = q.single_task<MainKernel>([=]() [[intel::kernel_args_restrict]] {\u000Aauto read_0 = pipes_ld_val_0::PipeAt<0>::read();\u000Aauto read_1 = pipes_ld_val_0::PipeAt<1>::read();\u000Apipes_st_val_0::PipeAt<0>::write({});\u000Apipe_end_lsq_signal_0::write({});\u000A    int matrix[kM * kM];\u000A#if TEST\u000A    for (int i = 0; i < (M * M); ++i)\u000A      matrix[i] = matrix_dram[i];\u000A#endif\u000A    int ptr = 0;\u000A    for (int k = 1; k < kM; k++) {\u000A      for (int p = 0; p < kM; p++) {\u000A        matrix[row[ptr]] += a[p] * matrix[col[ptr]];\u000A        ptr++;\u000A      }\u000A    }\u000A#if TEST\u000A    for (int i = 0; i < (M * M); ++i)\u000A      matrix_dram[i] = matrix[i];\u000A#endif\u000A  });\u000AlsqEvent_0.wait();\u000Aevent_MainKernel_AGU_0.wait();\u000A\u000A  event.wait();\u000A  q.copy(matrix_dram, h_matrix.data(), h_matrix.size()).wait();\u000A  sycl::free(matrix_dram, q);\u000A  sycl::free(row, q);\u000A  sycl::free(col, q);\u000A  sycl::free(a, q);\u000A  auto start = event.get_profiling_info<info::event_profiling::command_start>();\u000A  auto end = event.get_profiling_info<info::event_profiling::command_end>();\u000A  double time_in_ms = static_cast<double>(end - start) / 1000000;\u000A  return time_in_ms;\u000A}\u000Avoid spmv_cpu(std::vector<int> &matrix, const std::vector<int> &row, const std::vector<int> &col, std::vector<int> &a, const int M) {\u000A  int ptr = 0;\u000A  for (int k = 1; k < M; k++) {\u000A    for (int p = 0; p < M; p++) {\u000A      matrix[row[ptr]] += a[p] * matrix[col[ptr]];\u000A      ptr++;\u000A    }\u000A  }\u000A}\u000Avoid init_data(std::vector<int> &matrix, std::vector<int> &a, std::vector<int> &col_index, std::vector<int> &row_ptr, const uint M, const uint percentage) {\u000A  std::default_random_engine generator;\u000A  std::uniform_int_distribution<int> distribution(1, 99);\u000A  auto dice = std::bind(distribution, generator);\u000A  for (int i = 0; i < M * M; ++i) {\u000A    col_index[i] = (dice() < percentage) ? 1 : i;\u000A    row_ptr[i] = col_index[i];\u000A  }\u000A  std::fill(matrix.begin(), matrix.end(), 1);\u000A  std::fill(a.begin(), a.end(), 1);\u000A}\u000Aint main(int argc, char *argv[]) {\u000A  int ARRAY_SIZE = kM;\u000A  int PERCENTAGE = 0;\u000A  try {\u000A    if (argc > 1) {\u000A      ARRAY_SIZE = int(atoi(argv[1]));\u000A    }\u000A    if (argc > 2) {\u000A      PERCENTAGE = int(atoi(argv[2]));\u000A      if (PERCENTAGE < 0 || PERCENTAGE > 100)\u000A        throw std::invalid_argument(\"Invalid percentage.\");\u000A    }\u000A  } catch (exception const &e) {\u000A    std::cout << \"Incorrect argv.\\nUsage:\\n\"\u000A              << \"  ./executable [ARRAY_SIZE] [PERCENTAGE (% of iterations \"\u000A                 \"with dependencies.)]\\n\";\u000A    std::terminate();\u000A  }\u000A#if FPGA_SIM\u000A  auto d_selector = sycl::ext::intel::fpga_simulator_selector_v;\u000A#elif FPGA_HW\u000A  auto d_selector = sycl::ext::intel::fpga_selector_v;\u000A#else\u000A  auto d_selector = sycl::ext::intel::fpga_emulator_selector_v;\u000A#endif\u000A  try {\u000A    property_list properties{property::queue::enable_profiling()};\u000A    queue q(d_selector, exception_handler, properties);\u000A    std::cout << \"Running on device: \" << q.get_device().get_info<info::device::name>() << \"\\n\";\u000A    std::vector<int> matrix(kM * kM);\u000A    std::vector<int> golden_matrix(kM * kM);\u000A    std::vector<int> a(kM);\u000A    std::vector<int> row_ptr(kM * kM);\u000A    std::vector<int> col_index(kM * kM);\u000A    init_data(matrix, a, col_index, row_ptr, kM, PERCENTAGE);\u000A    std::copy(matrix.begin(), matrix.end(), golden_matrix.begin());\u000A    spmv_cpu(golden_matrix, row_ptr, col_index, a, kM);\u000A    auto kernel_time = spmv_kernel(q, matrix, row_ptr, col_index, a, kM);\u000A    std::cout << \"Kernel time (ms): \" << kernel_time << \"\\n\";\u000A#if TEST\u000A    if (std::equal(matrix.begin(), matrix.end(), golden_matrix.begin())) {\u000A      std::cout << \"Passed\\n\";\u000A    } else {\u000A      std::cerr << \"Failed\";\u000A    }\u000A#endif\u000A  } catch (exception const &e) {\u000A    std::cout << \"An exception was caught.\\n\";\u000A    std::terminate();\u000A  }\u000A  return 0;\u000A}"}, {"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/tuple.hpp", "name":"tuple.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/tuple.hpp", "content":"#ifndef __TUPLE_HPP__\u000A#define __TUPLE_HPP__\u000A\u000A#include <type_traits>\u000A\u000Anamespace fpga_tools {\u000A\u000A//\u000A// Generic tuple\u000A//\u000A// USAGE EXAMPLE:\u000A//    Tuple<char,short,int,long> my_tuple;\u000A//    char a = my_tuple.get<0>();\u000A//    short b = my_tuple.get<1>();\u000A//    int c = my_tuple.get<2>();\u000A//    long d = my_tuple.get<3>();\u000A//\u000Atemplate <typename... Tys>\u000Astruct Tuple {\u000A  Tuple(Tys... Args) : values(Args...) {}\u000A  Tuple() {}\u000A\u000A  //\u000A  // get the index'th item in the tuple of values\u000A  //\u000A  template <int index>\u000A  auto& get() {\u000A    static_assert(index < NumTys, \"index out of bounds\");\u000A    return get_impl<index, Tys...>(values);\u000A  }\u000A\u000A  //\u000A  // helper to get the first element in the tuple\u000A  //\u000A  auto& first() { return get<0>(); }\u000A\u000A  //\u000A  // helper to get the last element in the tuple\u000A  //\u000A  auto& last() { return get<NumTys - 1>(); }\u000A\u000A private:\u000A  //\u000A  // generic tuple implementation: recursive case\u000A  //\u000A  template <typename CurrentTy, typename... OtherTys>\u000A  struct tuple_impl {\u000A    tuple_impl(CurrentTy& current, OtherTys... others)\u000A        : value(current), other_values(others...) {}\u000A    tuple_impl() {}\u000A\u000A    using ValueTy = CurrentTy;\u000A    ValueTy value;\u000A    tuple_impl<OtherTys...> other_values;\u000A  };\u000A\u000A  //\u000A  // generic tuple implementation: base case\u000A  //\u000A  template <typename FinalTy>\u000A  struct tuple_impl<FinalTy> {\u000A    tuple_impl(FinalTy& current) : value(current) {}\u000A    tuple_impl() {}\u000A\u000A    using ValueTy = FinalTy;\u000A    ValueTy value;\u000A  };\u000A\u000A  // the tuple values\u000A  tuple_impl<Tys...> values;\u000A\u000A  // the number of tuple values\u000A  constexpr static auto NumTys = sizeof...(Tys);\u000A\u000A  //\u000A  // implementation of 'get' for general tuple\u000A  //\u000A  template <int index, typename HeadTy, typename... TailTys>\u000A  static auto& get_impl(tuple_impl<HeadTy, TailTys...>& sub_tuple) {\u000A    if constexpr (index == 0) {\u000A      // base case\u000A      return sub_tuple.value;\u000A    } else {\u000A      // recursive case\u000A      return get_impl<index - 1, TailTys...>(sub_tuple.other_values);\u000A    }\u000A  }\u000A};\u000A\u000A//\u000A// NTuple implementation\u000A// This is convenient way to have N elements of the same type\u000A// somewhat like an std::array\u000A//\u000Atemplate <int, typename Type>\u000Ausing NTupleElem = Type;\u000A\u000Atemplate <typename Type, std::size_t... Idx>\u000Astatic auto make_NTupleImpl(std::index_sequence<Idx...>)\u000A    -> Tuple<NTupleElem<Idx, Type>...>;\u000A\u000Atemplate <int N, typename Type>\u000Ausing make_NTuple =\u000A    decltype(make_NTupleImpl<Type>(std::make_index_sequence<N>()));\u000A\u000A//\u000A// convenience alias for a tuple of N elements of the same type\u000A// NOTE: for this alias, typename comes FIRST (to match std::array)\u000A//\u000A// USAGE EXAMPLE:\u000A//    NTuple<int, 10> elements;\u000A//    elements.get<3>() = 17;\u000A//\u000Atemplate <typename Type, int N>\u000Ausing NTuple = make_NTuple<N, Type>;\u000A\u000A}  // namespace fpga_tools\u000A\u000A#endif /* __TUPLE_HPP__ */"}, {"path":"/home/u119070/git/elastic-sycl-hls/include_sycl/unrolled_loop.hpp", "name":"unrolled_loop.hpp", "has_active_debug_locs":false, "absName":"/home/u119070/git/elastic-sycl-hls/include_sycl/unrolled_loop.hpp", "content":"#ifndef __UNROLLEDLOOP_HPP__\u000A#define __UNROLLEDLOOP_HPP__\u000A\u000A#include <type_traits>\u000A#include <utility>\u000A\u000A#include \"metaprogramming_utils.hpp\"\u000A\u000Anamespace fpga_tools {\u000A///////////////////////////////////////////////////////////////////////////////\u000A//\u000A// Example usage for UnrolledLoop constexpr:\u000A//\u000A// Base\u000A//    UnrolledLoop(std::integer_sequence<int,5,2,7,8>{},[&](auto i) {\u000A//      /* i = 5,2,7,8 */\u000A//    });\u000A//\u000A// Case A\u000A//    UnrolledLoop<10>([&](auto i) {\u000A//      /* i = 0,1,...,9 */\u000A//    });\u000A//\u000A// Case B\u000A//    UnrolledLoop<10>([&](auto i) {\u000A//      /* i = 0,1,...,9 */\u000A//    });\u000A//\u000A// Case C\u000A//    UnrolledLoop<char, 1, 10>([&](auto i) {\u000A//      /* i = 1,2,...,9 */\u000A//    });\u000A//    UnrolledLoop<char, 10, 1>([&](auto i) {\u000A//      /* i = 10,9,...,2 */\u000A//    });\u000A//\u000A// Case D\u000A//    UnrolledLoop<1, 10>([&](auto i) {\u000A//      /* i = 1,2,...,9 */\u000A//    });\u000A//    UnrolledLoop<10, 1>([&](auto i) {\u000A//      /* i = 10,9,...,2 */\u000A//    });\u000A//\u000A///////////////////////////////////////////////////////////////////////////////\u000A\u000A//\u000A// Base implementation\u000A// Templated on:\u000A//    ItType    - the type of the iterator (size_t, int, char, ...)\u000A//    ItType... - the indices to iterate on\u000A//    F         - the function to run for each index (i.e. the lambda)\u000A//\u000Atemplate <class ItType, ItType... inds, class F>\u000Aconstexpr void UnrolledLoop(std::integer_sequence<ItType, inds...>, F&& f) {\u000A  (f(std::integral_constant<ItType, inds>{}), ...);\u000A}\u000A\u000A//\u000A// Convience implementation (A)\u000A// performs UnrolledLoop in range [0,n) with iterator of type ItType\u000A//\u000Atemplate <class ItType, ItType n, class F>\u000Aconstexpr void UnrolledLoop(F&& f) {\u000A  UnrolledLoop(std::make_integer_sequence<ItType, n>{}, std::forward<F>(f));\u000A}\u000A\u000A//\u000A// Convenience implementation (B)\u000A// performs UnrolledLoop in range [0,n) with an iterator of type std::size_t\u000A//\u000Atemplate <std::size_t n, class F>\u000Aconstexpr void UnrolledLoop(F&& f) {\u000A  UnrolledLoop(std::make_index_sequence<n>{}, std::forward<F>(f));\u000A}\u000A\u000A//\u000A// Convenience implementation (C)\u000A// performs UnrolledLoop from start...end with an iterator of type ItType\u000A// NOTE:  start is INCLUSIVE, end is EXCLUSIVE\u000A// NOTE:  if start<=end, sequence is start,start+1,...,end-1\u000A//        if end<=start, sequence is start,start-1,...,end+1\u000A//\u000Atemplate <class ItType, ItType start, ItType end, class F>\u000Aconstexpr void UnrolledLoop(F&& f) {\u000A  UnrolledLoop(make_integer_range<ItType, start, end>{}, std::forward<F>(f));\u000A}\u000A\u000A//\u000A// Convenience implementation (D)\u000A// performs UnrolledLoop from start...end with an iterator of type size_t\u000A// NOTE:  start is INCLUSIVE, end is EXCLUSIVE\u000A// NOTE:  if start<=end, sequence is start,start+1,...,end-1\u000A//        if end<=start, sequence is start,start-1,...,end+1\u000A//\u000Atemplate <std::size_t start, std::size_t end, class F>\u000Aconstexpr void UnrolledLoop(F&& f) {\u000A  UnrolledLoop(make_index_range<start, end>{}, std::forward<F>(f));\u000A}\u000A\u000A}  // namespace fpga_tools\u000A\u000A#endif /* __UNROLLEDLOOP_HPP__ */"}]